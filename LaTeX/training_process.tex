\chapterauthor{Karl Thyssen}
In reinforcement learning an agent is placed into an environment where it learns to interact with it dependent on the weighting of the 'trainer' on certain desired characteristics. This optimal policy is determined by the rewards the trainer supplies the agent with, which are based on the state of the environment at the time the agent is to make a decision. Through regular updates of the policy($\pi$) after receiving a reward for games played based on the current policy the agent will begin to favour actions it predicts will lead to higher rewards hence improve the policy and bring it as closely as possible to the optimum.

Formally this is known as the Markov-Decision-Process where the state of the environment at a particular step t in the game is represented as $s_t$ and the action taken for this time step t is represented as $a_t$. The current state is input into the current policy which leads the agent to select its next action. The action is selected from the action space $\mathcal{A} = $\state{\text{UP}, \text{LEFT}, \text{DOWN}, \text{RIGHT}, \text{BOMB}, \text{WAIT}}. The reward for this action $r_t$ is then given by the reward function $r(s_t, a_t)$ given which the agent can then evaluate this move for the update of its policy. In Bomberman its simple to see that the state $s_t$ is given by the distribution of agents, loot, bombs, coins etc. in the game map represented as the trainer wishes. 

In Bomberman an action can not be rewarded entirely isolated from the game as a whole as actions have direct repercussions least 6 steps into the future (by the time the explosion has dissipated) and even beyond. Therefore the action must be rewarded based on all future steps with those very far in the future having decreasing impact. Therefore we introduce a discount factor $\gamma \in [0, 1]$. We arbitrarily selected 0.9 to ensure the steps $n+4, n+5, n+6$ have a non-negligible impact on the reward, as these are the steps that we hope to cash in from a bomb and the agent should see that this was a well placed bomb if this is the case. Unfortunately we see this being very negative as the agent often self-destructs...

Therefore the reward function that the agent is wishing to maximise the expected long term reward $E[\text{Reward}|\pi]$ for the optimal policy is: $$\text{Reward} = \sum_{t=0}^{\infty} \gamma^t r_{t+1}$$. 

	\subsection{Q-Learning}
	\chapterauthor{Karl Thyssen}
	With our selected state vector of length 532 it would be highly impractical of not impossible to calculate the state action pairs for every state and the expected reward of each action due to the massive amount of data required for this along with the computationally intensive time requirement. Therefore we use the approach of Q-Learning to find the optimal Q function for each state-action pair in the data, updated using the discounted reward received for the action. We do this in the callback function of the agent when \code{end\_of\_episode()} is called, the discounted rewards are filled into the places of the previously stored immediate rewards of each state-action pair. 
	
	We store our Q-function as 6 random forest regressors; one for each action, from which the agent will choose an action based on the stochastic policy of the trained forests, each supplying the expected reward for the action it is trained for. This action $a_{t+1} = \pi(a_t|s_t)$ will, in training be mapped onto the calculated reward along with all state action pairs in the 10,000 games played during each particular policy iteration to retrain the forests, that is to say update the Q-function.
	
	\subsection{Exploration and exploitation}

	\chapterauthor{Hein-Erik Schnell, Karl Thyssen}
	While optimising the Q function based solely (exploitative approach) on the games of the previous generation (the trees trained on the past 10,000 games) it is possible to slip into a local maximum in terms of the yield which may well not be the optimal Q. Therefore we introduced a degree of exploration using an $\epsilon$-greedy policy in which a random action is performed with the probability $\epsilon \in [0,1]$. We selected $\epsilon = 0.25$. \par
	
	\subsubsection{Max-Boltzman-exploration}
	When exploration occurrs (not the best action is chosen), we decided not to choose the action completely random. Instead, choosing more promising actions should be more likely. This means that the expected rewards need to be mapped onto probabilities with which the respective action is chosen. The probability is given by 
	\begin{align}
		\pi (s,a) = \frac{e^{Q(s,a)/T}}{\sum_{a} e^{Q(s,a)/T}}\text{,}
	\end{align}
	where $\pi (s,a)$ is the probability that the action $a$ is chosen given state $s$, $Q(s,a)$ is the expected reward if action a is performed after state $s$ occured, $a \in \mathcal{A}\setminus a_{max}$ are all available actions after the action with the highest expected reward $a_{max}$ has been deleted from the set of actions $\mathcal{A}$ because $a_{max}$ would have been chosen if we didn't explore, and $T$ is a temperature-parameter which scales the ratios between the probabilities. We found that $T$ should be of about the same magnitude as the expected rewards which is why we chose $T$ as the mean of the absolute values of the expected rewards. Figure \ref{MB_table} shows an example of this.
	\begin{figure}[h]
		\centering
		\begin{tabular}{c|c}
			$Q(s,a)$ & $\pi (s,a)$\\
			\midrule
			$43$ & $0.52$\\
			$21$ & $0.25$\\
			$4$ & $0.14$\\
			$-19$ & $0.07$\\
			$-65$ & $0.02$
		\end{tabular}
	\caption{\textit{Exemplary table of expected rewards $Q(s,a)$ and corresponding probabilities $\pi (s,a)$ if $T$ is chosen as described above. In this case: $T=30.4$}}
	\label{MB_table}
	\end{figure}
	We later decided to enable \textit{exploration} only in one fourth of all rounds. The reason is that the agent can only survive after it placed a bomb if it moves consistently away from it or around a corner. Thus, a random move in every fourth step may be fatal. With the above measure, we could at least ensure the survival after placing a bomb and make the agent learn this behaviour.
	
	\subsection{Training data}
	\chapterauthor{Karl Thyssen}
	We generated new training data after each training generation of 10,000 games to train the new forests that is to say the Q function for the next generation to learn from. Therefore the size of our final training data array of action state vectors was dependent on the number of rounds the agent survived in a given generation. We saved each set of training data independently of each other to allow for training from any generation and later access to the data. The random forest regressors were also saved to allow for analysis of them and comparison between the generations by allowing play from any generation and even play against younger generation. Ideally play against younger generations would always lead to free wins as the agent should be significantly better after some time, however our agent is a lonely agent that rarely sees others anywhere other than in its state vectors.
